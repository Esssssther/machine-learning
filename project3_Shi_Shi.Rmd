---
title: "R Notebook"
output: github_document
---

This is an [R Markdown](http://rmarkdown.rstudio.com) Notebook. When you execute code within the notebook, the results appear beneath the code. 

Try executing this chunk by clicking the *Run* button within the chunk or by placing your cursor inside it and pressing *Cmd+Shift+Enter*. 

## Library

```{r, message=FALSE, warning=FALSE}
options(scipen = 999)
library(tidymodels)
library(tidyverse)
library(janitor)
library(vip)
library(skimr)
library(reshape2)
```

#import data 

```{r}
boston <- read_csv("boston_train.csv") %>% clean_names()
kaggle <- read_csv("boston_holdout.csv") %>% clean_names()

head(boston)
boston %>% skim()
```

```{r}
sprintf("rice rule bins=%d",floor((9959^(1/3))*2))

ggplot(boston, aes(x = av_total)) + 
  geom_histogram(bins = 43, col= "white") +
  labs(title=" Histogram of assessed value for property",x="av_total",y="count")
boston %>%
  ggplot(aes(y=av_total))+
  geom_boxplot()+
  labs(title="box plot of assessed value for property",y="av_total")

ggplot(boston, aes(x = av_total)) + 
  geom_histogram(bins = 43, col= "white") +
  scale_x_log10() +
  labs(title="Histogram Log of assessed value for property",x="av_total",y="log of count")
```
```{r}
boston<-boston %>%
  mutate(yr_remod = replace_na(yr_remod,0))%>%
  mutate(age=if_else(yr_remod>yr_built, 2022-yr_remod,2022-yr_built))

skim(boston)

kaggle<-kaggle%>%
  mutate(yr_remod = replace_na(yr_remod,0))%>%
  mutate(age=if_else(yr_remod>yr_built, 2022-yr_remod,2022-yr_built))
```


# explore category variables
```{r}
boston_character<-select_if(boston, is.character)

for (col in colnames(boston_character)){
    print(boston%>%
    ggplot(aes(y=!!as.name(col),x=av_total))+
    geom_boxplot()+
    labs(title=paste("comparing",col,"with av_total"),x="av_total",y=col))

}
```


# explore numeric variables

```{r}
boston%>%
  select_if(is.numeric) %>%
  select(-pid,-zipcode,-yr_built,-yr_remod) %>%
  cor() %>%
  melt() %>%
  filter(Var1 =="av_total") %>%
  ggplot(aes(Var1,Var2, fill=value)) +
  geom_tile() +
  scale_fill_gradient2(mid="#FBFEF9",low="#0C6291",high="#A63446")+
  geom_text(aes(label=round(value,3)), color="white")+
  labs(title = "all variable's correlation")

```
```{r}

boston%>%
  ggplot(aes(x=living_area,y=av_total))+
  geom_point()+
  geom_smooth(method=lm)

boston%>%
  ggplot(aes(x=av_total , y=factor(median_income)))+
  geom_boxplot()


boston%>%
  ggplot(aes(x=age,y=av_total))+
  geom_point()+
  geom_smooth(method=lm)

boston%>%
  filter(yr_built>0 & yr_remod==0)%>%
  ggplot(aes(x=yr_built,y=av_total,na.rm=TRUE))+ 
  geom_point()+
  geom_smooth(method=lm)


  ggplot(boston[which(boston$yr_remod>0),],aes(x=yr_remod,y=av_total,na.rm=TRUE))+ 
  geom_point()+
    geom_smooth(method=lm)

model_built<-lm(av_total~ yr_built, data= boston[(boston$yr_built>0 & boston$yr_remod==0),])  
summary(model_built)

model_age<-lm(av_total~ age, data = boston)
summary(model_age)

model_remod<-lm(av_total~ yr_remod, data= boston[which(boston$yr_remod>0),])  
summary(model_remod)
```


```{r}

set.seed(1)
# Save the split information for an 80/20 split of the data
bsplit <- initial_split(boston, prop = 0.8)
train <- training(bsplit) 
test  <-  testing(bsplit)

# Kfold cross validation
kfold_splits <- vfold_cv(train, v=5)

```


```{r}
# write out the formula 
## Change variables here
boston_recipe <-
  recipe(av_total ~ land_sf + living_area + age + num_floors + own_occ+ city_state + r_ovrall_cnd +
           r_int_cnd +r_total_rms+median_income+r_full_bth+r_ext_cnd+r_kitch_style+r_bth_style+r_ext_fin+r_bldg_styl+r_half_bth+r_roof_typ
           , data = train) %>%
  step_impute_median(all_numeric_predictors()) %>% # missing values numeric 
  step_novel(all_nominal_predictors()) %>% # new factor levels 
  step_unknown(all_nominal_predictors()) %>% # missing values 
  step_dummy(all_nominal_predictors(), one_hot = TRUE) %>%
  step_nzv(all_predictors())

## Check the recipe results m
bake(boston_recipe %>% prep(),train %>% sample_n(1000))

## every thing should be numeric, regression problems, don't make av_total a factor

```
```{r}
lm_model <- linear_reg(mixture = 1, penalty = 0.01) %>%
  set_engine("glm") %>%  #other engine such as glm, lm dont have mixture and penalty, lm is the basic one
  set_mode("regression") 

lm_wflow <-workflow() %>%
  add_recipe(boston_recipe) %>%
  add_model(lm_model) %>%
  fit(train)

tidy(lm_wflow) %>%
  mutate_if(is.numeric,round,4)

lm_wflow %>%
  pull_workflow_fit() %>%
  vi() %>%
  mutate(Importance = if_else(Sign=="NEG",-Importance,Importance))%>%
  ggplot(aes(x=reorder(Variable,Importance),y=Importance,fill=Sign))+
  geom_col()+
  coord_flip()+
  labs(title="linear regression model importance plot")
  
#It's a regression model, do not have type of prob or class, only have type of numeric  
bind_cols(
  predict(lm_wflow,train,type="numeric"), train) %>% 
 mutate(part="train")->score_lm_train

bind_cols(
  predict(lm_wflow,test), test) %>% 
  mutate(part="test")->score_lm_test

bind_rows(score_lm_train,score_lm_test)%>%
  group_by(part) %>%
  metrics(av_total,.pred)%>%
  pivot_wider(id_cols = part,names_from = .metric,values_from = .estimate)
```
# XG boost

```{r}

xgb_model <- boost_tree(trees=tune(), 
                        learn_rate = tune(),
                        tree_depth = tune()) %>%
  set_engine("xgboost",
             importance="permutation") %>%
  set_mode("regression")


xgb_wflow <-workflow() %>%
  add_recipe(boston_recipe) %>%
  add_model(xgb_model)


xgb_search_res <- xgb_wflow %>% 
  tune_bayes(
    resamples = kfold_splits,
    # Generate five at semi-random to start
    initial = 5,
    iter = 10, 
    # How to measure performance?
    metrics = metric_set(rmse, rsq),
    control = control_bayes(no_improve = 10, verbose = TRUE)
  )# if no improvement after 10 iterations, stop grid
```


```{r}
xgb_search_res %>%
  collect_metrics()%>%
  filter(.metric == "rmse")

# Graph of learning rate 
xgb_search_res %>%
  collect_metrics() %>%
  ggplot(aes(learn_rate, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")

# graph of tree depth 
xgb_search_res %>%
  collect_metrics() %>%
  ggplot(aes(tree_depth, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")

# graph of number of trees 
xgb_search_res %>%
  collect_metrics() %>%
  ggplot(aes(trees, mean, color = .metric)) +
  geom_errorbar(aes(
    ymin = mean - std_err,
    ymax = mean + std_err
  ),
  alpha = 0.5
  ) +
  geom_line(size = 1.5) +
  facet_wrap(~.metric, scales = "free", nrow = 2) +
  scale_x_log10() +
  theme(legend.position = "none")
```


```{r}

lowest_xgb_rmse <- xgb_search_res %>%
  select_best("rmse")
lowest_xgb_rmse

xgb_wflow <- finalize_workflow(
  xgb_wflow, lowest_xgb_rmse
) %>% 
  fit(train)

```


## VIP 
What variables are important 
```{r}
xgb_wflow %>%
  extract_fit_parsnip() %>%
  vi()%>%
  ggplot(aes(x=reorder(Variable,Importance),y=Importance))+
  geom_col()+
  coord_flip()+
  labs(title="xgb model importance plot")
# use these models replug to your linear regression formula

xgb_wflow %>%
  extract_fit_parsnip() %>%
  vip(10)+
  labs(title="xgb modeltop 10 importance plot")
```

```{r}
bind_cols(
  predict(xgb_wflow,train), train) %>% 
  metrics(av_total,.pred)%>%
  mutate(part="train")

bind_cols(
  predict(xgb_wflow,test), test) %>% 
  metrics(av_total,.pred)%>%
  mutate(part="test")
```

## Best Worst Predicitons 

You should have one best and two worst predictions 

1. the properties that you under-estimate the value of
2. the properties that you over-estimate the value of 
3. the properties that are your best-estimate 

```{r}


bind_cols(predict(xgb_wflow,test),test) %>%
  mutate(error = av_total - .pred,
         abs_error = abs(error)) %>% 
  slice_min(order_by = abs_error,n=10) ->best_estimate

best_estimate

# best estimate 
bind_cols(predict(xgb_wflow,test),test) %>%
  mutate(error = av_total - .pred,
         ) %>% 
  slice_max(order_by = error,n=10) ->under_estimate

under_estimate


# overly simplistic evaluation 


# worst over-estimate 
bind_cols(predict(xgb_wflow,test),test)%>%
  mutate(error = .pred-av_total ) %>% 
  slice_max(order_by = error,n=10) -> overesimate

overesimate
# overly simplistic evaluation 

best_estimate %>% 
  summarize(
    mean(error),
    mean(av_total),
            mean(yr_built))

overesimate %>% 
  summarize(
    mean(error),
    mean(av_total),
            mean(yr_built))
under_estimate%>%
    summarize(
    mean(error),
    mean(av_total),
            mean(yr_built))

```

## KAGGLE 
```{r}
bind_cols(predict(xgb_wflow,kaggle),kaggle) %>%
  select(pid,av_total = .pred) %>% 
  write_csv("project3_kaggle_50484.csv")
```



#Random Forest
```{r}
rf_model <- rand_forest(trees=tune()) %>%
  set_engine("ranger",
             importance="permutation") %>%
  set_mode("regression")


rf_wflow <-workflow() %>%
  add_recipe(boston_recipe) %>%
  add_model(rf_model)


rf_search_res <- rf_wflow %>% 
  tune_bayes(
    resamples = kfold_splits,
    # Generate five at semi-random to start
    initial = 5,
    iter = 10, 
    # How to measure performance?
    metrics = metric_set(rmse, rsq),
    control = control_bayes(no_improve = 10, verbose = TRUE)
  )
```



```{r}
rf_search_res %>%
  collect_metrics()%>%
  filter(.metric == "rmse")




```


```{r}

lowest_rf_rmse <- rf_search_res %>%
  select_best("rmse")
lowest_rf_rmse

rf_wflow <- finalize_workflow(
  rf_wflow, lowest_rf_rmse
) %>% 
  fit(train)

```

## VIP 
What variables are important 
```{r}
rf_wflow %>%
  extract_fit_parsnip() %>%
  vi()%>%
  ggplot(aes(x=reorder(Variable,Importance),y=Importance))+
  geom_col()+
  coord_flip()+
  labs(title="random forest model importance plot")
# use these models replug to your linear regression formula
```

```{r}
bind_cols(
  predict(rf_wflow,train), train) %>% 
  metrics(av_total,.pred)%>%
  mutate(part="train")

bind_cols(
  predict(rf_wflow,test), test) %>% 
  metrics(av_total,.pred)%>%
  mutate(part="test")
```


Add a new chunk by clicking the *Insert Chunk* button on the toolbar or by pressing *Cmd+Option+I*.

When you save the notebook, an HTML file containing the code and output will be saved alongside it (click the *Preview* button or press *Cmd+Shift+K* to preview the HTML file). 

The preview shows you a rendered HTML copy of the contents of the editor. Consequently, unlike *Knit*, *Preview* does not run any R code chunks. Instead, the output of the chunk when it was last run in the editor is displayed.

